---
title: "Classification"
output: html_notebook
---

```{r}
library("data.table")
library("caret")
library("ggplot2")
library("corrplot")
```

Load data
```{r}
BreastCancer <- fread("data/breast-cancer-wisconsin.data")
colnames(BreastCancer) <- c("ID", "ClumpThick", "UniSize", "UniShape", "MargAdh", "SingEpiCellSize", "BareNuclei", "BlandChr", "NormalNucl", "Mitoses", "Class")
```

```{r}
# Quick check of data
summary(BreastCancer)
table(BreastCancer$Class) # unequal distribution of classes

BreastCancer$BareNuclei <- as.integer(BreastCancer$BareNuclei)
CorPlot <- cor(BreastCancer[,2:10], method = "pearson", use = "complete.obs")
highlyCorrelated <- findCorrelation(CorPlot, cutoff = 0.75)
highlyCorrelated # need to remove UniSize
corrplot.mixed(CorPlot, order = "hclust")

BreastCancer$Class <- as.factor(BreastCancer$Class)
```
```{r}
# Checking if there is NA values
sapply(BreastCancer, function(x) sum(is.na(x))) # 16 in BareNuclei column, those rows will be omitted


# Setting the random seed number for reproducibility
set.seed(123)

# Stratisfied random split of data into training and test data set
Index <- createDataPartition(BreastCancer$Class, p = 0.8, list = FALSE)
TrainingSet <- BreastCancer[Index, c(1:2,4:11)] # Removed UniSize
TestSet <- BreastCancer[-Index, c(1:2,4:11)]
```

```{r}
ggplot(TrainingSet, aes(x = ClumpThick, y = UniShape)) + geom_jitter(color = "red") + geom_jitter(data = TestSet, aes(x = ClumpThick, y = UniShape), color = "blue") + theme_light()
```

Support Vector Machine (SVM model)
```{r}
# Building training model
Model <- train(Class ~ .-ID, data = TrainingSet,
                    method = "svmPoly",
                    na.action = na.omit,
                    preProcess = c("scale", "center"),
                    trControl = trainControl(method = "none"),
                    tuneGrid = data.frame(degree = 1, scale = 1, C = 1)
                    )

# Building CV model
CVModel <- train(Class ~ .-ID, data = TrainingSet,
                    method = "svmPoly",
                    na.action = na.omit,
                    preProcess = c("scale", "center"),
                    trControl = trainControl(method = "cv", number = 10),
                    tuneGrid = data.frame(degree = 1, scale = 1, C = 1)
                    )

# Predict with the built model
PredictedTrainSet <- predict(Model, TrainingSet)
PredictedTestSet <- predict(Model, TestSet)
CV <- predict(CVModel, TrainingSet)

# How well the model predicts?
PerfTrainPred <- confusionMatrix(PredictedTrainSet, TrainingSet[BareNuclei != "NA", Class])
PerfTrainPred #Accuracy: 0.9726

PerfTestPred <- confusionMatrix(PredictedTestSet, TestSet[BareNuclei != "NA", Class])
PerfTestPred # Accuracy: 0.9778

PerfCVPred <- confusionMatrix(CV, TrainingSet[BareNuclei != "NA", Class])
PerfCVPred # Accuracy: 0.9726

# Feature importance
FeatImport <- varImp(Model)
plot(FeatImport)
```

Logistic regression as a classification method
```{r}
# data
GLM_data <- copy(BreastCancer)
GLM_data[, Class := ifelse(Class == "2", 0, 1)]

TrainingSetGLM <- GLM_data[Index, ][!(is.na(BareNuclei))]
TestSetGLM <- GLM_data[-Index, ][!(is.na(BareNuclei))]

# Model
GLMModel <- glm(Class ~ . -ID, family = binomial, data = TrainingSetGLM, na.action = "na.omit")

summary(GLMModel)

# Predict with model
LogPredictTrain <- predict(GLMModel, TrainingSetGLM, type = "response")
PredClassTrainGLM <- as.numeric(ifelse(LogPredictTrain > 0.5, 1, 0))

LogPredictTest <- predict(GLMModel, TestSetGLM, type = "response")
PredClassTestGLM <- as.numeric(ifelse(LogPredictTest > 0.5, 1, 0))

# How well does the model predicts?
confusionMatrix(as.factor(PredClassTrainGLM), as.factor(TrainingSetGLM$Class)) # Accuracy 0.9782
confusionMatrix(as.factor(PredClassTestGLM), as.factor(TestSetGLM$Class)) # Accuracy 0.9624
```